\title{\vspace{-3cm}A justifiable prior for U-INVITE}
\author{Jeff Zemla}
\date{\today}

\documentclass[12pt]{article}
\usepackage{bbm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}

\begin{document}
\maketitle

\section{The zero-inflated beta-binomial}

The prior for an edge, $\mathbb{P}(G^{-s}_{ij})$, is constructed around the observed counts $\beta_o$ and $\alpha_o$ which denote the number of participants (excluding the participant $s$ being fitted) who either do or do not have that edge ($G^n_{ij}$) respectively. Specifically,

\begin{equation}
    \beta_o = \sum_{n \neq s}^N \mathbbm{1}_{G^n_{ij}=1}
\end{equation}

\begin{equation}
    \alpha_o = \sum_{n \neq s}^N \mathbbm{1}_{G^n_{ij}=0}
\end{equation}

\noindent
where $N$ is the total number of participants, $s$ is the participant whose graph is being estimated, and $G^n_{ij}$ denotes an edge (or lack of edge) between $i$ and $j$ for participant $n$. Note that $\alpha_o$ is not necessarily $N - \beta_o$ (and vice versa) because a participant's graph might not contain both items $i$ and $j$;  thus the edge is undefined for that participant, rather than zero. \\

It is assumed that these counts are generated from a zero-inflated beta-binomial process. Zeros are generated with probability $p_1$, while zeros and ones are estimated from a beta-binomial ($BB$) process with probability $1 - p_1$. The reason for this is that with a sparsity of data for each participant, any estimated graph will be too sparse, i.e., contain too many zeros compared to the true graph. Since each uncensored walk traverses only a small fraction of the total number of edges, there is no data to estimate edges that are not traversed, leading to excess zeros. This is the ``zero-inflated" portion of the model, whereas the $BB$ process models the ``signal" in the observed counts, i.e., a true estimate of the edge prior from the U-INVITE process. \\

The observed proportion of zeros is

\begin{equation}
    \label{eq:observedmu2}
    \mu_2 = \frac{\alpha_o}{\alpha_o + \beta_o}
\end{equation}

\noindent
and

\begin{equation}
    \label{eq:mus}
    \mu_2 = p_1 + (1-p_1)\mu_1
\end{equation}

\noindent
so

\begin{equation}
    \label{eq:solveformu1}
    \mu_1  = \frac{\mu_2 - p_1}{1 - p_1}
\end{equation}

\noindent
Substituting Equation \ref{eq:observedmu2} into Equation \ref{eq:solveformu1} we have

\begin{equation}
    \mu_1 = \frac{\alpha_o - p_1(\alpha_o + \beta_o)}{(\alpha_o + \beta_o)(1 - p_1)}
\end{equation}

\noindent
or equivalently,

\begin{equation}
    \mu_1 = \frac{\alpha_o - p_1(\alpha_o + \beta_o)}{\alpha_o + \beta_o - p_1(\alpha_o + \beta_o)}
\end{equation} \\ 

In other words, $\mu_1$ represents the proportion of zeros in the counts after subtracting zeros that were not modeled by the $BB$ process. The prior for an edge is then

\begin{equation}
    \label{eq:priorwp1}
    P(G^{-s}_{ij}) = 1 - \mu_1 = 1 - \Bigg[\frac{\alpha_o - p_1n}{(1 - p_1)n}\Bigg]
\end{equation}

\noindent
where $n = \alpha_o + \beta_o$. \\

There is an additional constraint such that $p_1n \leq \alpha_o$; with greater values of $p_1$ it would not be possible to observe $\alpha_o$ zeros. To enforce this constraint, we scale the probability of the zero-generating process to range from zero to $u_2$:

\begin{equation}
    \label{eq:p1}
    p_1 = p_2 \frac{\alpha_o}{\alpha_o + \beta_o}
\end{equation}

\noindent
where $p_2$ denotes the proportion of observed zeros that were due to the zero-generating process. Now $p_2$ is a free parameter that can range from zero to one. By substituting Equation \ref{eq:p1} into Equation \ref{eq:priorwp1} we can now represent the prior on an edge as:

\begin{equation}
    \label{eq:fullprior}
    P(G^{-s}_{ij}) = 1 - \Bigg[\frac{\alpha_o(1 - p_2)}{n - p_2\alpha_o}\Bigg]
\end{equation}

\noindent
With some substitution, we can simplify this equation:

\begin{equation}
    \label{eq:fullprior}
    P(G^{-s}_{ij}) = \frac{\beta}{\beta + \alpha_o(1-p_2)}
\end{equation}

\section{Unresolved issues}

The prior in Equation \ref{eq:fullprior} seems to work fine. $p_2 = .275$ seems to emulate the unjustified ``halfa'' prior used earlier. There are a few issues that would be nice to resolve if possible. One issue is that the results are fairly sensitive to the choice for $p_2$, and it would be nice if there were a principled process for choosing $p_2$ based on the data. Another related issue is that $p_2$ really shouldn't be a fixed number at all. By assuming a fixed value for $p_2$ we are saying that, no matter what a graph looks like, a fixed proportion of the non-edges are spurious (i.e., resulting from the zero-generating process). But we are modeling this process, so the graph becomes less sparse over time. Therefore, $p_2$ should reduce to zero as the density of the estimated graph approaches the density of the true graph. In addition, the process cannot model an excess of ones in the counts, i.e., when the estimated graph is too dense. This might happen if a na√Øve random walk is used to estimate the initial graph and each participant has a moderate amount of data. \\

I did try to model this such that $p_2$ denotes the proportion of excess zeros in the data given the current prior graph density $d$ and the desired or prior density of the true graph, $D$:

\begin{equation}
    p_2 = \frac{D-d}{1-d}
\end{equation} \\

\noindent
$p_2$ dynamically changes as the graph density changes. This eliminates a free parameter and simultaneously allows for modeling of graphs that are too dense. Unfortunately, it doesn't work. Or rather, it's noticeably worse than fixing the parameter as is done above.

\end{document}
